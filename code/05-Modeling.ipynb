{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, plot_confusion_matrix\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.base import TransformerMixin # this allows to create a custom transformer\n",
    "from sklearn.ensemble import RandomForestClassifier,\\\n",
    "                             ExtraTreesClassifier,\\\n",
    "                             GradientBoostingClassifier,\\\n",
    "                             AdaBoostClassifier,\\\n",
    "                             VotingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5995, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>text_all</th>\n",
       "      <th>y</th>\n",
       "      <th>texts_cleaned</th>\n",
       "      <th>post_w_cnt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>relationship_advice</td>\n",
       "      <td>BI 58 Male Caught With Guy I'am a bi 58 year o...</td>\n",
       "      <td>1</td>\n",
       "      <td>bi  male caught with guy i am a year old  have...</td>\n",
       "      <td>139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>legaladvice</td>\n",
       "      <td>When a dealer gets arrested do his roommates g...</td>\n",
       "      <td>0</td>\n",
       "      <td>when a dealer get arrested do his roommate get...</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>relationship_advice</td>\n",
       "      <td>My(14M) friend (14F) that I’ve been talking to...</td>\n",
       "      <td>1</td>\n",
       "      <td>my m friend f that i ve been talking to just t...</td>\n",
       "      <td>184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>relationship_advice</td>\n",
       "      <td>Did my boyfriend and I make the right assumpti...</td>\n",
       "      <td>1</td>\n",
       "      <td>did my boyfriend and i make the right assumpti...</td>\n",
       "      <td>356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>relationship_advice</td>\n",
       "      <td>Tough patch in our relationship of four years ...</td>\n",
       "      <td>1</td>\n",
       "      <td>tough patch in our relationship of four year m...</td>\n",
       "      <td>206</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             subreddit                                           text_all  y  \\\n",
       "0  relationship_advice  BI 58 Male Caught With Guy I'am a bi 58 year o...  1   \n",
       "1          legaladvice  When a dealer gets arrested do his roommates g...  0   \n",
       "2  relationship_advice  My(14M) friend (14F) that I’ve been talking to...  1   \n",
       "3  relationship_advice  Did my boyfriend and I make the right assumpti...  1   \n",
       "4  relationship_advice  Tough patch in our relationship of four years ...  1   \n",
       "\n",
       "                                       texts_cleaned  post_w_cnt  \n",
       "0  bi  male caught with guy i am a year old  have...         139  \n",
       "1  when a dealer get arrested do his roommate get...          36  \n",
       "2  my m friend f that i ve been talking to just t...         184  \n",
       "3  did my boyfriend and i make the right assumpti...         356  \n",
       "4  tough patch in our relationship of four year m...         206  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in csv\n",
    "\n",
    "df = pd.read_csv('/Users/juhee/Desktop/GA/Submissions/Projects/project_3-master/data/reddit_final.csv')\n",
    "\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "subreddit        0\n",
       "text_all         0\n",
       "y                0\n",
       "texts_cleaned    0\n",
       "post_w_cnt       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check null values \n",
    "\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up X and y and train test split\n",
    "\n",
    "X = df['texts_cleaned']\n",
    "y = df['y']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .33, random_state = 42, stratify = y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.744821\n",
       "0    0.255179\n",
       "Name: y, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# establish baseline accuracy\n",
    "\n",
    "y_test.value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CountVectorizer()\n",
    "#### Multinomial Naive Bayes and Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set 2 pipelines with: 1. Multinomial Bayes 2. Logistic Regression\n",
    "\n",
    "pipeline_nb = Pipeline([\n",
    "                       ('cvec', CountVectorizer()),\n",
    "                       ('mb', MultinomialNB())\n",
    "                       ])\n",
    "\n",
    "pipeline_log = Pipeline([\n",
    "                        ('cvec', CountVectorizer()),\n",
    "                        ('logreg', LogisticRegression())\n",
    "                        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up a dictionary to store the best parameters\n",
    "\n",
    "model_nb_params = {}\n",
    "count_nb = 0\n",
    "\n",
    "model_log_params = {}\n",
    "count_log = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search over the following values of hyperparameters:\n",
    "# maximum number of features fit: \n",
    "# 5500, 6000, 6500, 7000 -> 5000, 5500, 6500: the maximum number of features did not impact the score. will decrease to see if we get higher score\n",
    "# -> 4000, 4500, 5000, 5500, 6000 -> 4000, 5000, 5500, 6000 -> 2000, 3000, 4000, 5000   \n",
    "# minimum number of documents needed to include token\n",
    "# 2, 3 -> 1, 2 -> 1, 2, 3\n",
    "# maximum number of documents needed to include token\n",
    "# 85%, 87.5%, 90%, 92.5% -> 80%, 82.5%, 85%, 90%: : max number of documents keep going down, but the score does not change, indicating 85% is the limit. will increase\n",
    "# -> 85%, 90%, 95%\n",
    "# check (individual tokens) and also check (individual tokens and 2-grams).\n",
    "\n",
    "pipe_params = {\n",
    "    'cvec__max_features': [4000, 5000, 5500, 6000],\n",
    "    'cvec__min_df': [1, 2, 3],\n",
    "    'cvec__max_df': [.85, .90, .95],\n",
    "    'cvec__ngram_range': [(1, 1), (1, 2)],\n",
    "    'cvec__stop_words': ['english', None]\n",
    "}\n",
    "\n",
    "# scores did not change with variation of hyperparameters. \n",
    "# the best parameters - max feature: [5500, 6000], minimum documents: [1, 2], max documents: [0.85], ngram: (1, 1), stop words: [None, english]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate GridSearchCV - NaiveBayes\n",
    "\n",
    "gs_nb = GridSearchCV(pipeline_nb, \n",
    "                     param_grid = pipe_params, \n",
    "                     cv = 3) \n",
    "\n",
    "# instantiate GridSearchCV - LogisticRegression\n",
    "\n",
    "gs_log = GridSearchCV(pipeline_log, \n",
    "                      param_grid = pipe_params, \n",
    "                      cv = 3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# increase counter as we put in different parameters\n",
    "\n",
    "count_nb += 1\n",
    "\n",
    "# fit the training set\n",
    "\n",
    "gs_nb.fit(X_train, y_train)\n",
    "\n",
    "# best score is not captured in parameters -> create a key 'score' to store the info\n",
    "\n",
    "gs_nb.best_params_['score'] = gs_nb.best_score_\n",
    "\n",
    "model_nb_params[f'{gs_nb}_{count_nb}'] = gs_nb.best_params_\n",
    "\n",
    "# dictionary to a dataframe\n",
    "\n",
    "model_nb_df = pd.DataFrame.from_dict(model_nb_params, orient = 'index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cvec__max_df</th>\n",
       "      <th>cvec__max_features</th>\n",
       "      <th>cvec__min_df</th>\n",
       "      <th>cvec__ngram_range</th>\n",
       "      <th>cvec__stop_words</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>GridSearchCV(cv=3,\\n             estimator=Pipeline(steps=[('cvec', CountVectorizer()),\\n                                       ('mb', MultinomialNB())]),\\n             param_grid={'cvec__max_df': [0.85, 0.875, 0.9, 0.925],\\n                         'cvec__max_features': [5500, 6000, 6500, 7000],\\n                         'cvec__min_df': [2, 3],\\n                         'cvec__ngram_range': [(1, 1), (1, 2)],\\n                         'cvec__stop_words': ['english', None]})_1</th>\n",
       "      <td>0.85</td>\n",
       "      <td>6500</td>\n",
       "      <td>2</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>None</td>\n",
       "      <td>0.937997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GridSearchCV(cv=3,\\n             estimator=Pipeline(steps=[('cvec', CountVectorizer()),\\n                                       ('mb', MultinomialNB())]),\\n             param_grid={'cvec__max_df': [0.8, 0.825, 0.85, 0.9],\\n                         'cvec__max_features': [5000, 5500, 6500, 6750],\\n                         'cvec__min_df': [1, 2],\\n                         'cvec__ngram_range': [(1, 1), (1, 2)],\\n                         'cvec__stop_words': ['english', None]})_2</th>\n",
       "      <td>0.80</td>\n",
       "      <td>6500</td>\n",
       "      <td>2</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>None</td>\n",
       "      <td>0.937997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GridSearchCV(cv=3,\\n             estimator=Pipeline(steps=[('cvec', CountVectorizer()),\\n                                       ('mb', MultinomialNB())]),\\n             param_grid={'cvec__max_df': [0.85, 0.9, 0.95],\\n                         'cvec__max_features': [4000, 4500, 5000, 5500, 6000],\\n                         'cvec__min_df': [1, 2],\\n                         'cvec__ngram_range': [(1, 1), (1, 2)],\\n                         'cvec__stop_words': ['english', None]})_4</th>\n",
       "      <td>0.85</td>\n",
       "      <td>6000</td>\n",
       "      <td>1</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>None</td>\n",
       "      <td>0.937997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GridSearchCV(cv=3,\\n             estimator=Pipeline(steps=[('cvec', CountVectorizer()),\\n                                       ('mb', MultinomialNB())]),\\n             param_grid={'cvec__max_df': [0.85, 0.9, 0.95],\\n                         'cvec__max_features': [4000, 5000, 5500, 6000],\\n                         'cvec__min_df': [1, 2, 3],\\n                         'cvec__ngram_range': [(1, 1), (1, 2)],\\n                         'cvec__stop_words': ['english', None]})_5</th>\n",
       "      <td>0.85</td>\n",
       "      <td>6000</td>\n",
       "      <td>1</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>None</td>\n",
       "      <td>0.937997</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    cvec__max_df  \\\n",
       "GridSearchCV(cv=3,\\n             estimator=Pipe...          0.85   \n",
       "GridSearchCV(cv=3,\\n             estimator=Pipe...          0.80   \n",
       "GridSearchCV(cv=3,\\n             estimator=Pipe...          0.85   \n",
       "GridSearchCV(cv=3,\\n             estimator=Pipe...          0.85   \n",
       "\n",
       "                                                    cvec__max_features  \\\n",
       "GridSearchCV(cv=3,\\n             estimator=Pipe...                6500   \n",
       "GridSearchCV(cv=3,\\n             estimator=Pipe...                6500   \n",
       "GridSearchCV(cv=3,\\n             estimator=Pipe...                6000   \n",
       "GridSearchCV(cv=3,\\n             estimator=Pipe...                6000   \n",
       "\n",
       "                                                    cvec__min_df  \\\n",
       "GridSearchCV(cv=3,\\n             estimator=Pipe...             2   \n",
       "GridSearchCV(cv=3,\\n             estimator=Pipe...             2   \n",
       "GridSearchCV(cv=3,\\n             estimator=Pipe...             1   \n",
       "GridSearchCV(cv=3,\\n             estimator=Pipe...             1   \n",
       "\n",
       "                                                   cvec__ngram_range  \\\n",
       "GridSearchCV(cv=3,\\n             estimator=Pipe...            (1, 1)   \n",
       "GridSearchCV(cv=3,\\n             estimator=Pipe...            (1, 1)   \n",
       "GridSearchCV(cv=3,\\n             estimator=Pipe...            (1, 1)   \n",
       "GridSearchCV(cv=3,\\n             estimator=Pipe...            (1, 1)   \n",
       "\n",
       "                                                   cvec__stop_words     score  \n",
       "GridSearchCV(cv=3,\\n             estimator=Pipe...             None  0.937997  \n",
       "GridSearchCV(cv=3,\\n             estimator=Pipe...             None  0.937997  \n",
       "GridSearchCV(cv=3,\\n             estimator=Pipe...             None  0.937997  \n",
       "GridSearchCV(cv=3,\\n             estimator=Pipe...             None  0.937997  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_nb_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# increase counter as we put in different parameters\n",
    "\n",
    "count_log += 1\n",
    "\n",
    "# fit the training set\n",
    "\n",
    "gs_log.fit(X_train, y_train)\n",
    "\n",
    "# best score is not captured in parameters -> create a key 'score' to store the info\n",
    "\n",
    "gs_log.best_params_['score'] = gs_log.best_score_\n",
    "\n",
    "model_log_params[f'{gs_log}_{count_log}'] = gs_log.best_params_\n",
    "\n",
    "# dictionary to a dataframe\n",
    "\n",
    "model_log_df = pd.DataFrame.from_dict(model_log_params, orient = 'index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cvec__max_df</th>\n",
       "      <th>cvec__max_features</th>\n",
       "      <th>cvec__min_df</th>\n",
       "      <th>cvec__ngram_range</th>\n",
       "      <th>cvec__stop_words</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>GridSearchCV(cv=3,\\n             estimator=Pipeline(steps=[('cvec', CountVectorizer()),\\n                                       ('logreg', LogisticRegression())]),\\n             param_grid={'cvec__max_df': [0.85, 0.875, 0.9, 0.925],\\n                         'cvec__max_features': [5500, 6000, 6500, 7000],\\n                         'cvec__min_df': [2, 3],\\n                         'cvec__ngram_range': [(1, 1), (1, 2)],\\n                         'cvec__stop_words': ['english', None]})_1</th>\n",
       "      <td>0.85</td>\n",
       "      <td>5500</td>\n",
       "      <td>2</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>english</td>\n",
       "      <td>0.917084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GridSearchCV(cv=3,\\n             estimator=Pipeline(steps=[('cvec', CountVectorizer()),\\n                                       ('logreg', LogisticRegression())]),\\n             param_grid={'cvec__max_df': [0.8, 0.825, 0.85, 0.9],\\n                         'cvec__max_features': [5000, 5500, 6500, 6750],\\n                         'cvec__min_df': [1, 2],\\n                         'cvec__ngram_range': [(1, 1), (1, 2)],\\n                         'cvec__stop_words': ['english', None]})_2</th>\n",
       "      <td>0.80</td>\n",
       "      <td>5500</td>\n",
       "      <td>2</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>english</td>\n",
       "      <td>0.917084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GridSearchCV(cv=3,\\n             estimator=Pipeline(steps=[('cvec', CountVectorizer()),\\n                                       ('logreg', LogisticRegression())]),\\n             param_grid={'cvec__max_df': [0.85, 0.9, 0.95],\\n                         'cvec__max_features': [4000, 4500, 5000, 5500, 6000],\\n                         'cvec__min_df': [1, 2],\\n                         'cvec__ngram_range': [(1, 1), (1, 2)],\\n                         'cvec__stop_words': ['english', None]})_3</th>\n",
       "      <td>0.85</td>\n",
       "      <td>5500</td>\n",
       "      <td>2</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>english</td>\n",
       "      <td>0.917084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GridSearchCV(cv=3,\\n             estimator=Pipeline(steps=[('cvec', CountVectorizer()),\\n                                       ('logreg', LogisticRegression())]),\\n             param_grid={'cvec__max_df': [0.85, 0.9, 0.95],\\n                         'cvec__max_features': [4000, 5000, 5500, 6000],\\n                         'cvec__min_df': [1, 2, 3],\\n                         'cvec__ngram_range': [(1, 1), (1, 2)],\\n                         'cvec__stop_words': ['english', None]})_4</th>\n",
       "      <td>0.85</td>\n",
       "      <td>5500</td>\n",
       "      <td>2</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>english</td>\n",
       "      <td>0.917084</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    cvec__max_df  \\\n",
       "GridSearchCV(cv=3,\\n             estimator=Pipe...          0.85   \n",
       "GridSearchCV(cv=3,\\n             estimator=Pipe...          0.80   \n",
       "GridSearchCV(cv=3,\\n             estimator=Pipe...          0.85   \n",
       "GridSearchCV(cv=3,\\n             estimator=Pipe...          0.85   \n",
       "\n",
       "                                                    cvec__max_features  \\\n",
       "GridSearchCV(cv=3,\\n             estimator=Pipe...                5500   \n",
       "GridSearchCV(cv=3,\\n             estimator=Pipe...                5500   \n",
       "GridSearchCV(cv=3,\\n             estimator=Pipe...                5500   \n",
       "GridSearchCV(cv=3,\\n             estimator=Pipe...                5500   \n",
       "\n",
       "                                                    cvec__min_df  \\\n",
       "GridSearchCV(cv=3,\\n             estimator=Pipe...             2   \n",
       "GridSearchCV(cv=3,\\n             estimator=Pipe...             2   \n",
       "GridSearchCV(cv=3,\\n             estimator=Pipe...             2   \n",
       "GridSearchCV(cv=3,\\n             estimator=Pipe...             2   \n",
       "\n",
       "                                                   cvec__ngram_range  \\\n",
       "GridSearchCV(cv=3,\\n             estimator=Pipe...            (1, 1)   \n",
       "GridSearchCV(cv=3,\\n             estimator=Pipe...            (1, 1)   \n",
       "GridSearchCV(cv=3,\\n             estimator=Pipe...            (1, 1)   \n",
       "GridSearchCV(cv=3,\\n             estimator=Pipe...            (1, 1)   \n",
       "\n",
       "                                                   cvec__stop_words     score  \n",
       "GridSearchCV(cv=3,\\n             estimator=Pipe...          english  0.917084  \n",
       "GridSearchCV(cv=3,\\n             estimator=Pipe...          english  0.917084  \n",
       "GridSearchCV(cv=3,\\n             estimator=Pipe...          english  0.917084  \n",
       "GridSearchCV(cv=3,\\n             estimator=Pipe...          english  0.917084  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_log_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial Naive Bayes: 0.9509462151394422\n",
      "\n",
      "Logistic Regression: 0.9793326693227091\n"
     ]
    }
   ],
   "source": [
    "# train set score\n",
    "\n",
    "print(f'Multinomial Naive Bayes: {gs_nb.score(X_train, y_train)}')\n",
    "print(f'\\nLogistic Regression: {gs_log.score(X_train, y_train)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial Naive Bayes: 0.9388580090955028\n",
      "\n",
      "Logistic Regression: 0.9236988377968671\n"
     ]
    }
   ],
   "source": [
    "# test set score\n",
    "\n",
    "print(f'Multinomial Naive Bayes: {gs_nb.score(X_test, y_test)}')\n",
    "print(f'\\nLogistic Regression: {gs_log.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial Naive Bayes: \n",
      "{'cvec__max_df': 0.85, 'cvec__max_features': 6000, 'cvec__min_df': 1, 'cvec__ngram_range': (1, 1), 'cvec__stop_words': None, 'score': 0.9379972560563793}\n",
      "\n",
      "Logistic Regression: \n",
      "{'cvec__max_df': 0.85, 'cvec__max_features': 5500, 'cvec__min_df': 2, 'cvec__ngram_range': (1, 1), 'cvec__stop_words': 'english', 'score': 0.9170837096301853}\n"
     ]
    }
   ],
   "source": [
    "# find the best parameters for both models\n",
    "\n",
    "print(f'Multinomial Naive Bayes: \\n{gs_nb.best_params_}')\n",
    "print(f'\\nLogistic Regression: \\n{gs_log.best_params_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial Naive Bayes: \n",
      "Pipeline(steps=[('cvec', CountVectorizer(max_df=0.85, max_features=6000)),\n",
      "                ('mb', MultinomialNB())])\n",
      "\n",
      "Logistic Regression: \n",
      "Pipeline(steps=[('cvec',\n",
      "                 CountVectorizer(max_df=0.85, max_features=5500, min_df=2,\n",
      "                                 stop_words='english')),\n",
      "                ('logreg', LogisticRegression())])\n"
     ]
    }
   ],
   "source": [
    "# find the best estimators for both models\n",
    "\n",
    "print(f'Multinomial Naive Bayes: \\n{gs_nb.best_estimator_}')\n",
    "print(f'\\nLogistic Regression: \\n{gs_log.best_estimator_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<blockquote>Although the best accuracy score of Logistic Regression on train dataset was 98%, it failed to perform on test dataset - it only scored 92% - signaling its overfit on train dataset. On the other hand, Multinomial Niave Bayes performed consistently without much variation from best, train to test. Our best model here is <b>Multinomial Naive Bayes</b> model with estimators of <b>max document 0.85, max features 6,000</b>, and <b>minimum document of 1</b> and with a parameter of <b>ngram range 1</b> and <b>no stop words</b>.</blockquote>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions\n",
    "\n",
    "y_preds = gs_nb.predict(X_test)\n",
    "\n",
    "# confusion matrix values\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_preds).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# view confusion matrix\n",
    "\n",
    "plot_confusion_matrix(gs_nb, X_test, y_test, cmap = 'Blues', normalize = 'true');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest and Extra Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set 2 pipelines with: 3. Random Forest 4. Extra Trees\n",
    "\n",
    "# these 2 models are computationally exhausting (taking too long to run), so plugged in parameters instead using parameters\n",
    "# 1. n_estimators = 125 -> 250 -> 500 -> 1000\n",
    "# 2. max_depth = 4 -> 8 -> 16 -> 32\n",
    "# 3. min_samples_split = 4 -> 8 -> 16 -> 32\n",
    "# 4. random_state = 42\n",
    "\n",
    "pipeline_rf = Pipeline([\n",
    "                       ('cvec', CountVectorizer()),\n",
    "                       ('rf', RandomForestClassifier(n_estimators = 1000, max_depth = 32, min_samples_split = 32, random_state = 42))\n",
    "                       ])\n",
    "\n",
    "pipeline_et = Pipeline([\n",
    "                        ('cvec', CountVectorizer()),\n",
    "                        ('et', ExtraTreesClassifier(n_estimators = 1000, max_depth = 32, min_samples_split = 32, random_state = 42))\n",
    "                        ])\n",
    "\n",
    "# random forest score: \n",
    "# 0.75 -> 0.79 -> 0.85 -> 0.88\n",
    "# extra trees score:\n",
    "# 0.74 -> 0.77 -> 0.84 -> 0.88"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inserted the best parameters from Multinomial Bayes and Logistic Regression\n",
    "\n",
    "pipe_params = {\n",
    "    'cvec__max_features': [5500, 6000], # -> 5500\n",
    "    'cvec__min_df': [1, 2], # -> 1\n",
    "    'cvec__max_df': [.85, .90, .95], # -> 0.85\n",
    "    'cvec__ngram_range': [(1, 1)],\n",
    "    'cvec__stop_words': ['english', None] # -> None\n",
    "               }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate GridSearchCV - RandomForest\n",
    "\n",
    "gs_rf = GridSearchCV(pipeline_rf, \n",
    "                     param_grid = pipe_params, \n",
    "                     cv = 3) \n",
    "\n",
    "# instantiate GridSearchCV - ExtraTrees\n",
    "\n",
    "gs_et = GridSearchCV(pipeline_et, \n",
    "                      param_grid = pipe_params, \n",
    "                      cv = 3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.880729433539743\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'cvec__max_df': 0.85,\n",
       " 'cvec__max_features': 5500,\n",
       " 'cvec__min_df': 1,\n",
       " 'cvec__ngram_range': (1, 1),\n",
       " 'cvec__stop_words': 'english'}"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the training set\n",
    "\n",
    "gs_rf.fit(X_train, y_train)\n",
    "\n",
    "print(gs_rf.best_score_)\n",
    "gs_rf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8814764455845913\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'cvec__max_df': 0.85,\n",
       " 'cvec__max_features': 5500,\n",
       " 'cvec__min_df': 2,\n",
       " 'cvec__ngram_range': (1, 1),\n",
       " 'cvec__stop_words': 'english'}"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the training set\n",
    "\n",
    "gs_et.fit(X_train, y_train)\n",
    "\n",
    "print(gs_et.best_score_)\n",
    "gs_et.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<blockquote>These 2 models - Random Forest and Extra Trees - are computationally exhausting (expnesive) to run, since they were taking very long time. As I increased the numbers in their parameters, I was able to get better results, however, it also meant that much longer time. Both of these two did not render better results than Multinomial Naive Bayes or Logistic Regression.\n",
    "    </blockquote>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AdaBoost, GradientBoost, Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up empty lists to store the scores and parameters\n",
    "\n",
    "score = []\n",
    "par = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiate voting classifier\n",
    "\n",
    "vote = VotingClassifier([\n",
    "    ('ada', AdaBoostClassifier()),\n",
    "    ('gb', GradientBoostingClassifier()),\n",
    "    ('tree', DecisionTreeClassifier())\n",
    "])\n",
    "\n",
    "# create pipeline -> countvectorize, then vote\n",
    "\n",
    "pipeline_vt = Pipeline([\n",
    "                       ('cvec', CountVectorizer()),\n",
    "                       ('vote', vote)\n",
    "                       ])\n",
    "\n",
    "# set parameters\n",
    "# start with the best parameters from above models for countvectorizer\n",
    "\n",
    "params = {\n",
    "    'cvec__max_features': [5000, 5500], \n",
    "    'cvec__min_df': [1], \n",
    "    'cvec__max_df': [0.80, 0.85], \n",
    "    'cvec__ngram_range': [(1, 1), (1, 2)],\n",
    "    'cvec__stop_words': [None], \n",
    "    'vote__ada__n_estimators' : [400, 500],\n",
    "    'vote__gb__n_estimators' : [300, 400],\n",
    "    'vote__tree__max_depth' : [7, 8, 9]\n",
    "}\n",
    "\n",
    "# try grid search for the best parameters\n",
    "\n",
    "gs_vote = GridSearchCV(pipeline_vt,\n",
    "                  param_grid = params,\n",
    "                  cv = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3,\n",
       "             estimator=Pipeline(steps=[('cvec', CountVectorizer()),\n",
       "                                       ('vote',\n",
       "                                        VotingClassifier(estimators=[('ada',\n",
       "                                                                      AdaBoostClassifier()),\n",
       "                                                                     ('gb',\n",
       "                                                                      GradientBoostingClassifier()),\n",
       "                                                                     ('tree',\n",
       "                                                                      DecisionTreeClassifier())]))]),\n",
       "             param_grid={'cvec__max_df': [0.8, 0.85],\n",
       "                         'cvec__max_features': [5000, 5500],\n",
       "                         'cvec__min_df': [1],\n",
       "                         'cvec__ngram_range': [(1, 1), (1, 2)],\n",
       "                         'cvec__stop_words': [None],\n",
       "                         'vote__ada__n_estimators': [400, 500],\n",
       "                         'vote__gb__n_estimators': [300, 400],\n",
       "                         'vote__tree__max_depth': [7, 8, 9]})"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the training dataset\n",
    "\n",
    "gs_vote.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append score and parameter values\n",
    "\n",
    "score.append(gs_vote.best_score_)\n",
    "par.append(gs_vote.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9096135891817028\n",
      "{'cvec__max_df': 0.8, 'cvec__max_features': 5500, 'cvec__min_df': 1, 'cvec__ngram_range': (1, 1), 'cvec__stop_words': None, 'vote__ada__n_estimators': 400, 'vote__gb__n_estimators': 400, 'vote__tree__max_depth': 9}\n"
     ]
    }
   ],
   "source": [
    "print(gs_vote.best_score_)\n",
    "print(gs_vote.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<blockquote>Voting classifier takes the longest time to run. Though this model performs better than RandomForest or ExtraTrees, this model is neither the most efficient to run nor the best performing model.</blockquote>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate SVM\n",
    "\n",
    "svc = SVC()\n",
    "\n",
    "# create a pipeline: countvectorize then svc\n",
    "\n",
    "pipeline_svc = Pipeline([\n",
    "    ('cvec', CountVectorizer()),\n",
    "    ('svc', SVC())\n",
    "])\n",
    "\n",
    "# define parameters: start with the best parameters from multinomial naive bayes and logistic regression\n",
    "\n",
    "params = {\n",
    "    'cvec__max_features': [6000], \n",
    "    'cvec__min_df': [2], \n",
    "    'cvec__max_df': [0.80], \n",
    "    'cvec__ngram_range': [(1, 1)],\n",
    "    'cvec__stop_words': [None], \n",
    "    'svc__kernel': ['linear', 'rbf', 'polynomial', 'sigmoid'],\n",
    "    'svc__C': [3.0],\n",
    "    'svc__degree': [3]\n",
    "}\n",
    "\n",
    "\n",
    "# generate a gridsearch model using pipeline and parameters\n",
    "\n",
    "gs_svc = GridSearchCV(pipeline_svc,\n",
    "                      params,\n",
    "                      cv = 3,\n",
    "                      verbose = 1\n",
    "                      ) # tried to run several jobs in parallel, but got an error saying that it will not run because of processing issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  12 out of  12 | elapsed:   22.5s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3,\n",
       "             estimator=Pipeline(steps=[('cvec', CountVectorizer()),\n",
       "                                       ('svc', SVC())]),\n",
       "             param_grid={'cvec__max_df': [0.8], 'cvec__max_features': [6000],\n",
       "                         'cvec__min_df': [2], 'cvec__ngram_range': [(1, 1)],\n",
       "                         'cvec__stop_words': [None], 'svc__C': [3.0],\n",
       "                         'svc__degree': [3],\n",
       "                         'svc__kernel': ['linear', 'rbf', 'polynomial',\n",
       "                                         'sigmoid']},\n",
       "             verbose=1)"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the gridsearch model to training dataset\n",
    "\n",
    "gs_svc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9046308420900262"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_svc.best_score_\n",
    "\n",
    "# 1. 0.9046308420900262\n",
    "# 2. 0.9051294703042712\n",
    "# 3. 0.9046308420900262 --> rbf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cvec__max_df': 0.8,\n",
       " 'cvec__max_features': 6000,\n",
       " 'cvec__min_df': 2,\n",
       " 'cvec__ngram_range': (1, 1),\n",
       " 'cvec__stop_words': None,\n",
       " 'svc__C': 3.0,\n",
       " 'svc__degree': 3,\n",
       " 'svc__kernel': 'rbf'}"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_svc.best_params_\n",
    "\n",
    "# 1. \n",
    "# {'cvec__max_df': 0.8,\n",
    "# 'cvec__max_features': 6000,\n",
    "# 'cvec__min_df': 2,\n",
    "# 'cvec__ngram_range': (1, 1),\n",
    "# 'cvec__stop_words': None,\n",
    "# 'svc__C': 3.0,\n",
    "# 'svc__degree': 3,\n",
    "# 'svc__kernel': 'rbf'}\n",
    "\n",
    "# will change the parameters to:\n",
    "# cvec__max_df: [0.75, 0.8], max_features: [6000, 6500], min_df: [1, 2],\n",
    "# svc__C: np.linspace(3, 4, num = 3), degree: [2, 3]\n",
    "\n",
    "# 2.\n",
    "# {'cvec__max_df': 0.8,\n",
    "# 'cvec__max_features': 6000,\n",
    "# 'cvec__min_df': 2,\n",
    "# 'cvec__ngram_range': (1, 1),\n",
    "# 'cvec__stop_words': None,\n",
    "# 'svc__C': 3.0,\n",
    "# 'svc__degree': 3,\n",
    "# 'svc__kernel': 'rbf'}\n",
    "\n",
    "# no change in best parameters -> limit the features and change the kernel\n",
    "\n",
    "# 3.\n",
    "# same parameters with exception of kernel -> tried all kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9671314741035857"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_svc.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9196563921172309"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_svc.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<blockquote>Unlike my assumption that it might perform better than Multinomial Naive Bayes, its  best score is 90%. It scored better on the train dataset of 97%, however, the test dataset only scored 92% showing the model is overfit.</blockquote>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TfidfVectorizer()\n",
    "#### Multinomial Bayes and Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up 2 pipelines with: 1. Multinomial Naive Bayes 2. Logistic Regression\n",
    "\n",
    "pipe_nb_t = Pipeline([\n",
    "                      ('tvec', TfidfVectorizer()),\n",
    "                      ('nb', MultinomialNB())\n",
    "                     ])\n",
    "\n",
    "pipe_log_t = Pipeline([\n",
    "                       ('tvec', TfidfVectorizer()),\n",
    "                       ('log', LogisticRegression())\n",
    "                      ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up a dictionary to store the best parameters\n",
    "\n",
    "t_model_nb_params = {}\n",
    "t_count_nb = 0\n",
    "\n",
    "t_model_log_params = {}\n",
    "t_count_log = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search over the following values of hyperparameters:\n",
    "# maximum number of features fit: \n",
    "# 500, 1500, 2500, 3500 -> 1000, 1500, 2500, 3000 -> 1250, 1500, 2500, 2750\n",
    "# max_df:\n",
    "# 0.70, 0.75, 0.80 -> 0.6, 0.65, 0.70 -> 0.5, 0.6, 0.7\n",
    "# min_df:\n",
    "# 0.003, 0.004 -> 0.001, 0.002, 0.004\n",
    "# no stop words and english stop words\n",
    "# check (individual tokens) and also check (individual tokens and 2-grams) -> individual\n",
    "\n",
    "pipe_tvec_params = {\n",
    "    'tvec__max_features': [1250, 1500, 2500, 2750],\n",
    "    'tvec__stop_words': [None, 'english'],\n",
    "    'tvec__ngram_range': [(1, 1)],\n",
    "    'tvec__min_df': [0.001, 0.002, 0.004],\n",
    "    'tvec__max_df': [0.5, 0.6, 0.7]\n",
    "}\n",
    "\n",
    "# not much difference for the models with different parameters. \n",
    "# best parameters: max_df 0.7, max features [1500, 2500], min_df [0.003, 0.004], stop words [none, english], ngram (1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate GridSearchCV\n",
    "\n",
    "gs_tvec_nb = GridSearchCV(pipe_nb_t,\n",
    "                          param_grid = pipe_tvec_params,\n",
    "                          cv = 5)\n",
    "\n",
    "gs_tvec_log = GridSearchCV(pipe_log_t,\n",
    "                           param_grid = pipe_tvec_params,\n",
    "                           cv = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# increase counter as we put in different parameters\n",
    "\n",
    "t_count_nb += 1\n",
    "\n",
    "# fit the training set\n",
    "\n",
    "gs_tvec_nb.fit(X_train, y_train)\n",
    "\n",
    "# best score is not captured in parameters -> create a key 'score' to store the info\n",
    "\n",
    "gs_tvec_nb.best_params_['score'] = gs_tvec_nb.best_score_\n",
    "\n",
    "t_model_nb_params[f'{gs_tvec_nb}_{t_count_nb}'] = gs_tvec_nb.best_params_\n",
    "\n",
    "# dictionary to a dataframe\n",
    "\n",
    "t_model_nb_df = pd.DataFrame.from_dict(t_model_nb_params, orient = 'index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tvec__max_df</th>\n",
       "      <th>tvec__max_features</th>\n",
       "      <th>tvec__min_df</th>\n",
       "      <th>tvec__ngram_range</th>\n",
       "      <th>tvec__stop_words</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>GridSearchCV(cv=5,\\n             estimator=Pipeline(steps=[('tvec', TfidfVectorizer()),\\n                                       ('nb', MultinomialNB())]),\\n             param_grid={'tvec__max_df': [0.7, 0.75, 0.8],\\n                         'tvec__max_features': [500, 1500, 2500, 3500],\\n                         'tvec__min_df': [0.003, 0.004],\\n                         'tvec__ngram_range': [(1, 1), (1, 2)],\\n                         'tvec__stop_words': [None, 'english']})_1</th>\n",
       "      <td>0.7</td>\n",
       "      <td>1500</td>\n",
       "      <td>0.003</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>english</td>\n",
       "      <td>0.916834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GridSearchCV(cv=5,\\n             estimator=Pipeline(steps=[('tvec', TfidfVectorizer()),\\n                                       ('nb', MultinomialNB())]),\\n             param_grid={'tvec__max_df': [0.6, 0.65, 0.7],\\n                         'tvec__max_features': [1000, 1500, 2500, 3000],\\n                         'tvec__min_df': [0.002, 0.003, 0.004, 0.005],\\n                         'tvec__ngram_range': [(1, 1), (1, 2)],\\n                         'tvec__stop_words': [None, 'english']})_2</th>\n",
       "      <td>0.6</td>\n",
       "      <td>1500</td>\n",
       "      <td>0.002</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>english</td>\n",
       "      <td>0.917581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GridSearchCV(cv=5,\\n             estimator=Pipeline(steps=[('tvec', TfidfVectorizer()),\\n                                       ('nb', MultinomialNB())]),\\n             param_grid={'tvec__max_df': [0.5, 0.6, 0.7],\\n                         'tvec__max_features': [1250, 1500, 2500, 2750],\\n                         'tvec__min_df': [0.001, 0.002, 0.004],\\n                         'tvec__ngram_range': [(1, 1)],\\n                         'tvec__stop_words': [None, 'english']})_4</th>\n",
       "      <td>0.5</td>\n",
       "      <td>1500</td>\n",
       "      <td>0.002</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>english</td>\n",
       "      <td>0.917581</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    tvec__max_df  \\\n",
       "GridSearchCV(cv=5,\\n             estimator=Pipe...           0.7   \n",
       "GridSearchCV(cv=5,\\n             estimator=Pipe...           0.6   \n",
       "GridSearchCV(cv=5,\\n             estimator=Pipe...           0.5   \n",
       "\n",
       "                                                    tvec__max_features  \\\n",
       "GridSearchCV(cv=5,\\n             estimator=Pipe...                1500   \n",
       "GridSearchCV(cv=5,\\n             estimator=Pipe...                1500   \n",
       "GridSearchCV(cv=5,\\n             estimator=Pipe...                1500   \n",
       "\n",
       "                                                    tvec__min_df  \\\n",
       "GridSearchCV(cv=5,\\n             estimator=Pipe...         0.003   \n",
       "GridSearchCV(cv=5,\\n             estimator=Pipe...         0.002   \n",
       "GridSearchCV(cv=5,\\n             estimator=Pipe...         0.002   \n",
       "\n",
       "                                                   tvec__ngram_range  \\\n",
       "GridSearchCV(cv=5,\\n             estimator=Pipe...            (1, 1)   \n",
       "GridSearchCV(cv=5,\\n             estimator=Pipe...            (1, 1)   \n",
       "GridSearchCV(cv=5,\\n             estimator=Pipe...            (1, 1)   \n",
       "\n",
       "                                                   tvec__stop_words     score  \n",
       "GridSearchCV(cv=5,\\n             estimator=Pipe...          english  0.916834  \n",
       "GridSearchCV(cv=5,\\n             estimator=Pipe...          english  0.917581  \n",
       "GridSearchCV(cv=5,\\n             estimator=Pipe...          english  0.917581  "
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_model_nb_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# increase counter as we put in different parameters\n",
    "\n",
    "t_count_log += 1\n",
    "\n",
    "# fit the training set\n",
    "\n",
    "gs_tvec_log.fit(X_train, y_train)\n",
    "\n",
    "# best score is not captured in parameters -> create a key 'score' to store the info\n",
    "\n",
    "gs_tvec_log.best_params_['score'] = gs_tvec_log.best_score_\n",
    "\n",
    "t_model_log_params[f'{gs_tvec_log}_{t_count_log}'] = gs_tvec_log.best_params_\n",
    "\n",
    "# dictionary to a dataframe\n",
    "\n",
    "t_model_log_df = pd.DataFrame.from_dict(t_model_log_params, orient = 'index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tvec__max_df</th>\n",
       "      <th>tvec__max_features</th>\n",
       "      <th>tvec__min_df</th>\n",
       "      <th>tvec__ngram_range</th>\n",
       "      <th>tvec__stop_words</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>GridSearchCV(cv=5,\\n             estimator=Pipeline(steps=[('tvec', TfidfVectorizer()),\\n                                       ('log', LogisticRegression())]),\\n             param_grid={'tvec__max_df': [0.7, 0.75, 0.8],\\n                         'tvec__max_features': [500, 1500, 2500, 3500],\\n                         'tvec__min_df': [0.003, 0.004],\\n                         'tvec__ngram_range': [(1, 1), (1, 2)],\\n                         'tvec__stop_words': [None, 'english']})_1</th>\n",
       "      <td>0.7</td>\n",
       "      <td>2500</td>\n",
       "      <td>0.004</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>None</td>\n",
       "      <td>0.916336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GridSearchCV(cv=5,\\n             estimator=Pipeline(steps=[('tvec', TfidfVectorizer()),\\n                                       ('log', LogisticRegression())]),\\n             param_grid={'tvec__max_df': [0.6, 0.65, 0.7],\\n                         'tvec__max_features': [1000, 1500, 2500, 3000],\\n                         'tvec__min_df': [0.002, 0.003, 0.004, 0.005],\\n                         'tvec__ngram_range': [(1, 1), (1, 2)],\\n                         'tvec__stop_words': [None, 'english']})_2</th>\n",
       "      <td>0.7</td>\n",
       "      <td>2500</td>\n",
       "      <td>0.004</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>None</td>\n",
       "      <td>0.916336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GridSearchCV(cv=5,\\n             estimator=Pipeline(steps=[('tvec', TfidfVectorizer()),\\n                                       ('log', LogisticRegression())]),\\n             param_grid={'tvec__max_df': [0.5, 0.6, 0.7],\\n                         'tvec__max_features': [1250, 1500, 2500, 2750],\\n                         'tvec__min_df': [0.001, 0.002, 0.004],\\n                         'tvec__ngram_range': [(1, 1)],\\n                         'tvec__stop_words': [None, 'english']})_3</th>\n",
       "      <td>0.7</td>\n",
       "      <td>2500</td>\n",
       "      <td>0.004</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>None</td>\n",
       "      <td>0.916336</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    tvec__max_df  \\\n",
       "GridSearchCV(cv=5,\\n             estimator=Pipe...           0.7   \n",
       "GridSearchCV(cv=5,\\n             estimator=Pipe...           0.7   \n",
       "GridSearchCV(cv=5,\\n             estimator=Pipe...           0.7   \n",
       "\n",
       "                                                    tvec__max_features  \\\n",
       "GridSearchCV(cv=5,\\n             estimator=Pipe...                2500   \n",
       "GridSearchCV(cv=5,\\n             estimator=Pipe...                2500   \n",
       "GridSearchCV(cv=5,\\n             estimator=Pipe...                2500   \n",
       "\n",
       "                                                    tvec__min_df  \\\n",
       "GridSearchCV(cv=5,\\n             estimator=Pipe...         0.004   \n",
       "GridSearchCV(cv=5,\\n             estimator=Pipe...         0.004   \n",
       "GridSearchCV(cv=5,\\n             estimator=Pipe...         0.004   \n",
       "\n",
       "                                                   tvec__ngram_range  \\\n",
       "GridSearchCV(cv=5,\\n             estimator=Pipe...            (1, 1)   \n",
       "GridSearchCV(cv=5,\\n             estimator=Pipe...            (1, 1)   \n",
       "GridSearchCV(cv=5,\\n             estimator=Pipe...            (1, 1)   \n",
       "\n",
       "                                                   tvec__stop_words     score  \n",
       "GridSearchCV(cv=5,\\n             estimator=Pipe...             None  0.916336  \n",
       "GridSearchCV(cv=5,\\n             estimator=Pipe...             None  0.916336  \n",
       "GridSearchCV(cv=5,\\n             estimator=Pipe...             None  0.916336  "
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_model_log_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial Naive Bayes: 0.9175808380265547\n",
      "\n",
      "Logistic Regression: 0.9163358177976866\n"
     ]
    }
   ],
   "source": [
    "# best scores\n",
    "\n",
    "print(f'Multinomial Naive Bayes: {gs_tvec_nb.best_score_}')\n",
    "print(f'\\nLogistic Regression: {gs_tvec_log.best_score_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial Naive Bayes: 0.9307768924302788\n",
      "\n",
      "Logistic Regression: 0.9489541832669323\n"
     ]
    }
   ],
   "source": [
    "# train data accuracy score \n",
    "\n",
    "print(f'Multinomial Naive Bayes: {gs_tvec_nb.score(X_train, y_train)}')\n",
    "print(f'\\nLogistic Regression: {gs_tvec_log.score(X_train, y_train)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial Naive Bayes: 0.9242041435068217\n",
      "\n",
      "Logistic Regression: 0.9242041435068217\n"
     ]
    }
   ],
   "source": [
    "# test data accuracy score \n",
    "\n",
    "print(f'Multinomial Naive Bayes: {gs_tvec_nb.score(X_test, y_test)}')\n",
    "print(f'\\nLogistic Regression: {gs_tvec_log.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial Naive Bayes: \n",
      "Pipeline(steps=[('tvec',\n",
      "                 TfidfVectorizer(max_df=0.5, max_features=1500, min_df=0.002,\n",
      "                                 stop_words='english')),\n",
      "                ('nb', MultinomialNB())])\n",
      "\n",
      "Logistic Regression: \n",
      "Pipeline(steps=[('tvec',\n",
      "                 TfidfVectorizer(max_df=0.7, max_features=2500, min_df=0.004)),\n",
      "                ('log', LogisticRegression())])\n"
     ]
    }
   ],
   "source": [
    "# best estimators\n",
    "\n",
    "print(f'Multinomial Naive Bayes: \\n{gs_tvec_nb.best_estimator_}')\n",
    "print(f'\\nLogistic Regression: \\n{gs_tvec_log.best_estimator_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial Naive Bayes: \n",
      "{'tvec__max_df': 0.5, 'tvec__max_features': 1500, 'tvec__min_df': 0.002, 'tvec__ngram_range': (1, 1), 'tvec__stop_words': 'english', 'score': 0.9175808380265547}\n",
      "\n",
      "Logistic Regression: \n",
      "{'tvec__max_df': 0.7, 'tvec__max_features': 2500, 'tvec__min_df': 0.004, 'tvec__ngram_range': (1, 1), 'tvec__stop_words': None, 'score': 0.9163358177976866}\n"
     ]
    }
   ],
   "source": [
    "# best parameters\n",
    "\n",
    "print(f'Multinomial Naive Bayes: \\n{gs_tvec_nb.best_params_}')\n",
    "print(f'\\nLogistic Regression: \\n{gs_tvec_log.best_params_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<blockquote>\n",
    "    Both of the models performed pretty well. Logistic regression model seems to be overfitted slightly. Multinomial Naive Bayes model would be the best model for the tfidf vectorization as well.\n",
    "    </blockquote>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions\n",
    "\n",
    "y_preds_t = gs_tvec_nb.predict(X_test)\n",
    "\n",
    "# confusion matrix values\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_preds_t).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# view confusion matrix\n",
    "\n",
    "plot_confusion_matrix(gs_tvec_nb, X_test, y_test, cmap = 'Blues', normalize = 'true');\n",
    "\n",
    "# this underperforms significantly compared to CountVectorizer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest and Extra Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set 2 pipelines with: 3. Random Forest 4. Extra Trees\n",
    "\n",
    "# these 2 models are computationally exhausting (taking too long to run), so plugged in parameters instead using parameters\n",
    "# 1. n_estimators = 125 -> 250 -> 500 -> 1000\n",
    "# 2. max_depth = 4 -> 8 -> 16 -> 32\n",
    "# 3. min_samples_split = 4 -> 8 -> 16 -> 32\n",
    "# 4. random_state = 42\n",
    "\n",
    "pipeline_rf_t = Pipeline([\n",
    "                       ('tvec', TfidfVectorizer()),\n",
    "                       ('rf', RandomForestClassifier(n_estimators = 125, max_depth = 4, min_samples_split = 4, random_state = 42))\n",
    "                       ])\n",
    "\n",
    "pipeline_et_t = Pipeline([\n",
    "                        ('tvec', TfidfVectorizer()),\n",
    "                        ('et', ExtraTreesClassifier(n_estimators = 250, max_depth = 8, min_samples_split = 8, random_state = 42))\n",
    "                        ])\n",
    "\n",
    "# random forest score: \n",
    "# 0.88 -> 0.83 -> 0.77\n",
    "# extra trees score:\n",
    "# 0.74 -> 0.77 -> 0.77"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inserted the best parameters from Multinomial Bayes and Logistic Regression\n",
    "\n",
    "tvec_params = {\n",
    "    'tvec__max_features': [1000, 1250], \n",
    "    'tvec__min_df': [0.001, 0.004, 0.005], \n",
    "    'tvec__max_df': [.4, .5], \n",
    "    'tvec__ngram_range': [(1, 1)],\n",
    "    'tvec__stop_words': ['english', None]\n",
    "               }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate GridSearchCV - RandomForest\n",
    "\n",
    "gs_rf_t = GridSearchCV(pipeline_rf_t, \n",
    "                     param_grid = tvec_params, \n",
    "                     cv = 3) \n",
    "\n",
    "# instantiate GridSearchCV - ExtraTrees\n",
    "\n",
    "gs_et_t = GridSearchCV(pipeline_et_t, \n",
    "                      param_grid = tvec_params, \n",
    "                      cv = 3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7771403523068812\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'tvec__max_df': 0.5,\n",
       " 'tvec__max_features': 1000,\n",
       " 'tvec__min_df': 0.005,\n",
       " 'tvec__ngram_range': (1, 1),\n",
       " 'tvec__stop_words': None}"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the training set\n",
    "\n",
    "gs_rf_t.fit(X_train, y_train)\n",
    "\n",
    "print(gs_rf_t.best_score_)\n",
    "gs_rf_t.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7724098962071881\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'tvec__max_df': 0.4,\n",
       " 'tvec__max_features': 1000,\n",
       " 'tvec__min_df': 0.004,\n",
       " 'tvec__ngram_range': (1, 1),\n",
       " 'tvec__stop_words': None}"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the training set\n",
    "\n",
    "gs_et_t.fit(X_train, y_train)\n",
    "\n",
    "print(gs_et_t.best_score_)\n",
    "gs_et_t.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<blockquote>These 2 models are consistently underperforming even with the tweaking in hyper parameters. These are not good models.</blockquote>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AdaBoost, GradientBoost, Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create empty lists to store score and parameters\n",
    "\n",
    "score_t = []\n",
    "par_t = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiate voting classifier\n",
    "\n",
    "vote_t = VotingClassifier([\n",
    "    ('ada', AdaBoostClassifier()),\n",
    "    ('gb', GradientBoostingClassifier()),\n",
    "    ('tree', DecisionTreeClassifier())\n",
    "])\n",
    "\n",
    "# create pipeline -> countvectorize, then vote\n",
    "\n",
    "pipeline_vt_t = Pipeline([\n",
    "                       ('tvec', TfidfVectorizer()),\n",
    "                       ('vote', vote)\n",
    "                       ])\n",
    "\n",
    "# set parameters\n",
    "# start with the best parameters from above models for countvectorizer\n",
    "\n",
    "t_params = {\n",
    "    'tvec__max_features': [5500, 6000], \n",
    "    'tvec__min_df': [1, 2], \n",
    "    'tvec__max_df': [.85, .90, .95], \n",
    "    'tvec__ngram_range': [(1, 1)],\n",
    "    'tvec__stop_words': ['english', None], \n",
    "    'vote__ada__n_estimators' : [200, 300, 400],\n",
    "    'vote__gb__n_estimators' : [100, 200, 300],\n",
    "    'vote__tree__max_depth' : [8]\n",
    "}\n",
    "\n",
    "# try grid search for the best parameters\n",
    "\n",
    "gs_vote = GridSearchCV(pipeline_vt_t,\n",
    "                  param_grid = t_params,\n",
    "                  cv = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the training dataset\n",
    "\n",
    "gs_vote_t.fit(X_train, y_train)\n",
    "\n",
    "## this takes very long time to run - hours on and still running - aborted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate SVM\n",
    "\n",
    "svc = SVC()\n",
    "\n",
    "# create a pipeline: countvectorize then svc\n",
    "\n",
    "pipeline_svc_t = Pipeline([\n",
    "    ('tvec', TfidfVectorizer()),\n",
    "    ('svc', SVC())\n",
    "])\n",
    "\n",
    "# define parameters\n",
    "\n",
    "params_t = {\n",
    "    'tvec__max_features': [5500], \n",
    "    'tvec__min_df': [0.002], \n",
    "    'tvec__max_df': [0.7], \n",
    "    'tvec__ngram_range': [(1, 1)],\n",
    "    'tvec__stop_words': [None], \n",
    "    'svc__kernel': ['polynomial', 'linear', 'sigmoid', 'rbf'],\n",
    "    'svc__C': [2],\n",
    "    'svc__degree': [3]\n",
    "}\n",
    "\n",
    "\n",
    "# generate a gridsearch model using pipeline and parameters\n",
    "\n",
    "gs_svc_t = GridSearchCV(pipeline_svc_t,\n",
    "                      params_t,\n",
    "                      cv = 5,\n",
    "                      verbose = 1\n",
    "                      ) # tried to run several jobs in parallel, but got an error saying that it will not run because of processing issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  20 out of  20 | elapsed:   46.8s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('tvec', TfidfVectorizer()),\n",
       "                                       ('svc', SVC())]),\n",
       "             param_grid={'svc__C': [2], 'svc__degree': [3],\n",
       "                         'svc__kernel': ['polynomial', 'linear', 'sigmoid',\n",
       "                                         'rbf'],\n",
       "                         'tvec__max_df': [0.7], 'tvec__max_features': [5500],\n",
       "                         'tvec__min_df': [0.002], 'tvec__ngram_range': [(1, 1)],\n",
       "                         'tvec__stop_words': [None]},\n",
       "             verbose=1)"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the model\n",
    "\n",
    "gs_svc_t.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9297841427978414"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_svc_t.best_score_\n",
    "\n",
    "# 1. 0.925298795515174\n",
    "# 2. 0.9277891457568413\n",
    "# 3. 0.9297841427978414"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'svc__C': 2,\n",
       " 'svc__degree': 3,\n",
       " 'svc__kernel': 'rbf',\n",
       " 'tvec__max_df': 0.7,\n",
       " 'tvec__max_features': 5500,\n",
       " 'tvec__min_df': 0.002,\n",
       " 'tvec__ngram_range': (1, 1),\n",
       " 'tvec__stop_words': None}"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_svc_t.best_params_\n",
    "\n",
    "# 1.\n",
    "#{'svc__C': 2.0,\n",
    "# 'svc__degree': 3,\n",
    "# 'svc__kernel': 'rbf',\n",
    "# 'tvec__max_df': 0.7,\n",
    "# 'tvec__max_features': 5500,\n",
    "# 'tvec__min_df': 0.002,\n",
    "# 'tvec__ngram_range': (1, 1),\n",
    "# 'tvec__stop_words': None}\n",
    "\n",
    "# 2.\n",
    "# {'svc__C': 2.0,\n",
    "# 'svc__degree': 3,\n",
    "# 'svc__kernel': 'rbf',\n",
    "# 'tvec__max_df': 0.7,\n",
    "# 'tvec__max_features': 5500,\n",
    "# 'tvec__min_df': 0.002,\n",
    "# 'tvec__ngram_range': (1, 1),\n",
    "# 'tvec__stop_words': None}\n",
    "# there is no change in parameters -> will compare kernels in SVC\n",
    "\n",
    "# 3.\n",
    "# {'svc__C': 2,\n",
    "# 'svc__degree': 3,\n",
    "# 'svc__kernel': 'rbf',\n",
    "# 'tvec__max_df': 0.7,\n",
    "# 'tvec__max_features': 5500,\n",
    "# 'tvec__min_df': 0.002,\n",
    "# 'tvec__ngram_range': (1, 1),\n",
    "# 'tvec__stop_words': None}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9972609561752988"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# score the train dataset\n",
    "\n",
    "gs_svc_t.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9358261748357757"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# score the test dataset\n",
    "\n",
    "gs_svc_t.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<blockquote>Its best score was 93% and it got almost 100% in training dataset. However, it only scored to 93.6% suggesting it doesn't predict as well to new data. </blockquote>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adapted from https://stackoverflow.com/questions/28384680/scikit-learns-pipeline-a-sparse-matrix-was-passed-but-dense-data-is-required\n",
    "\n",
    "class DenseTransformer(TransformerMixin):\n",
    "    \n",
    "    def fit(self, X, y = None, **fit_params):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y = None, **fit_params):\n",
    "        return X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define pipeline\n",
    "\n",
    "pipe_gb = Pipeline([\n",
    "    ('tvec', TfidfVectorizer()),\n",
    "    (\"_\", DenseTransformer()),\n",
    "    ('gb', GaussianNB())\n",
    "])\n",
    "\n",
    "# set hyperparameters\n",
    "\n",
    "params_gb = {\n",
    "        'tvec__max_features': [750],\n",
    "        'tvec__max_df': [0.25, 0.3, 0.35, 0.4, 0.45],\n",
    "        'tvec__min_df': [0.0006, 0.0007, 0.0008],\n",
    "        'tvec__ngram_range': [(1,1), (1,2)],\n",
    "        'tvec__stop_words': [None, 'english']}\n",
    "\n",
    "# create gridsearch model with pipeline\n",
    "\n",
    "gs_gb = GridSearchCV(pipe_gb,\n",
    "                     param_grid = params_gb,\n",
    "                     cv = 5,\n",
    "                     verbose = 1,\n",
    "                     scoring = 'accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 60 candidates, totalling 300 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 300 out of 300 | elapsed:  2.1min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('tvec', TfidfVectorizer()),\n",
       "                                       ('_',\n",
       "                                        <__main__.DenseTransformer object at 0x7fe6d4719f50>),\n",
       "                                       ('gb', GaussianNB())]),\n",
       "             param_grid={'tvec__max_df': [0.25, 0.3, 0.35, 0.4, 0.45],\n",
       "                         'tvec__max_features': [750],\n",
       "                         'tvec__min_df': [0.0006, 0.0007, 0.0008],\n",
       "                         'tvec__ngram_range': [(1, 1), (1, 2)],\n",
       "                         'tvec__stop_words': [None, 'english']},\n",
       "             scoring='accuracy', verbose=1)"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the model\n",
    "\n",
    "gs_gb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.855822382483597"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_gb.best_score_\n",
    "\n",
    "# 1.\n",
    "# 0.8329138863589897\n",
    "# 2.\n",
    "# 0.8548258086900491\n",
    "# 3.\n",
    "# 0.855822382483597\n",
    "# 4.\n",
    "# 0.855822382483597 --> no change in score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tvec__max_df': 0.45,\n",
       " 'tvec__max_features': 750,\n",
       " 'tvec__min_df': 0.0006,\n",
       " 'tvec__ngram_range': (1, 1),\n",
       " 'tvec__stop_words': None}"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_gb.best_params_\n",
    "\n",
    "# 1.\n",
    "# {'tvec__max_df': 0.5,\n",
    "# 'tvec__max_features': 1000,\n",
    "# 'tvec__min_df': 0.001,\n",
    "# 'tvec__ngram_range': (1, 2),\n",
    "# 'tvec__stop_words': None}\n",
    "\n",
    "# 2.\n",
    "# {'tvec__max_df': 0.5,\n",
    "# 'tvec__max_features': 750,\n",
    "# 'tvec__min_df': 0.001,\n",
    "# 'tvec__ngram_range': (1, 1),\n",
    "# 'tvec__stop_words': None}\n",
    "\n",
    "# 3.\n",
    "# {'tvec__max_df': 0.45,\n",
    "# 'tvec__max_features': 750,\n",
    "# 'tvec__min_df': 0.0008,\n",
    "# 'tvec__ngram_range': (1, 1),\n",
    "# 'tvec__stop_words': None}\n",
    "\n",
    "# 4.\n",
    "# {'tvec__max_df': 0.45,\n",
    "# 'tvec__max_features': 750,\n",
    "# 'tvec__min_df': 0.0006,\n",
    "# 'tvec__ngram_range': (1, 1),\n",
    "# 'tvec__stop_words': None} -> most of the parameters did not change from #3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<blockquote>I thought this would have better score, however, it only scored 86% and the score doesn't seem to improve even with the hyperparameters. </blockquote>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
